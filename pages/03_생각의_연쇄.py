import streamlit as st
from openai import OpenAI
from streamlit_chat import message
from dotenv import load_dotenv
import time

load_dotenv()
client = OpenAI()


def clear_history():
    st.session_state["past"] = []
    st.session_state["generated"] = []


if "mesasges" not in st.session_state:
    st.session_state["messages"] = []
if "past" not in st.session_state:
    st.session_state["past"] = []
if "generated" not in st.session_state:
    st.session_state["generated"] = []

if "logined" not in st.session_state.keys() or not st.session_state["logined"]:
    st.error("ğŸš¨ ë¡œê·¸ì¸ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”")
    st.stop()

if st.session_state["api_type"] == "open_ai":
    models = ["gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"]

st.set_page_config(
    page_title="Chain-of-Thought",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={},
)

hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

explanation_0, explanation_1, explanation_2 = st.columns([0.07, 10, 0.07])
with explanation_1:
    st.title("â›“ï¸ ChatGPTì™€ ìƒê°ì˜ ì—°ì‡„(Chain-of-Thought)")
    st.write(
        """
        <br>
        ìµœì‹  ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ ìƒê°ì˜ ì—°ì‡„ (Chain-of-Thought)ëŠ” ì»´í“¨í„°ê°€ <b>ì°¨ê·¼ì°¨ê·¼</b> ìƒê°í•˜ë„ë¡ í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.
        <br>
        ìƒê°ì˜ ì—°ì‡„ëŠ” <a href="https://arxiv.org/pdf/2205.11916.pdf" target="_blank">2022ë…„ì— ë„ì¿„ëŒ€/êµ¬ê¸€ ê³µë™ ì—°êµ¬ì§„ì´ ì œì•ˆí•œ ë°©ë²•</a>ì…ë‹ˆë‹¤.
        <br>
        ì´ ê¸°ë²•ì€ ìˆ˜í•™ê³¼ ê°™ì´ ì°¨ê·¼ì°¨ê·¼ ìƒê°í•˜ëŠ” ê²ƒì´ í•„ìš”í•œ ì˜ì—­ì—ì„œ ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒ ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
        <br>
        ì‚¬ëŒì˜ ìƒê° ë°©ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ, ë‹¨ê³„ì ìœ¼ë¡œ 'ì¶”ë¡ (Reasoning)'ì˜ ê³¼ì •ì„ ê±°ì¹˜ë„ë¡ í•¨ìœ¼ë¡œì¨, í° ë¬¸ì œë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‹µë³€ì„ í•˜ê²Œí•˜ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.
        <br>
        ì•„ì§ ì—°êµ¬ê°€ ê¾¸ì¤€íˆ ì§„í–‰ ì¤‘ì¸ <b>ë¯¸ì§€ì˜ ë¶„ì•¼</b>ì…ë‹ˆë‹¤! ì—¬ëŸ¬ë¶„ë“¤ë§Œì˜ <b>ë§ˆë²•ì˜ ë¬¸ì¥</b>ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!
        <br><br>
        """,
        unsafe_allow_html=True,
    )


st.sidebar.title("ì„¤ì •")

with st.sidebar:
    history_clear = st.button(label="ì´ˆê¸°í™”", on_click=clear_history)
    history_text_space = st.empty()

    if history_clear:
        with history_text_space:
            st.write("ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í•˜ê³  ìˆì–´ìš” ğŸ˜‰")
            time.sleep(3)
            history_text_space.write("")

    st.write(
        """
        <font size=2>ëª¨ë¸ ì„¤ì •</font>
        """,
        unsafe_allow_html=True,
    )
    model_option = st.selectbox("Select Model", models)
    with st.expander(label="Max Words"):
        if model_option in ["gpt-3.5-turbo-0613"]:
            max_tokens = st.slider(
                "Max Words",
                min_value=5,
                max_value=4000,
                value=1000,
                step=1,
                label_visibility="hidden",
                help="ë‹µë³€ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
            )
        else:
            max_tokens = st.slider(
                "Max Words",
                min_value=5,
                max_value=16000,
                value=1000,
                step=1,
                label_visibility="hidden",
                help="ë‹µë³€ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
            )
    with st.expander(label="Temperature"):
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=2.0,
            value=0.5,
            step=0.1,
            label_visibility="hidden",
            help="ë‹µë³€ì˜ ë¬´ì‘ìœ„ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì •í˜•í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ê³ , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
        )
    with st.expander(label="Top P"):
        top_p = st.slider(
            "Top P",
            min_value=0.1,
            max_value=1.0,
            step=0.1,
            value=1.0,
            label_visibility="hidden",
            help="ë‹¨ì–´ë¥¼ ì„ íƒ í•  ë•Œ ììœ ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. í´ìˆ˜ë¡ ë‹¤ì–‘í•œ ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.",
        )
    with st.expander(label="Presence Penalty"):
        presence_penalty = st.slider(
            "Presence Penalty",
            min_value=-2.0,
            max_value=2.0,
            value=0.0,
            step=0.1,
            label_visibility="hidden",
            help="ì§ˆë¬¸ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ìƒì„± í•  ì§€ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.",
        )
    with st.expander(label="Frequency Penalty"):
        frequency_penalty = st.slider(
            "Frequency Penalty",
            min_value=-2.0,
            max_value=2.0,
            value=0.0,
            step=0.1,
            label_visibility="hidden",
            help="ì§ˆë¬¸ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ìƒì„± í•  ì§€ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì§ˆë¬¸ ë¬¸ì¥ì— ì‚¬ìš©ëœ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        )


submit_background, submit_cot = False, False
background_prompt, cot_prompt = "", ""

ce, c1, ce, c2, c3 = st.columns([0.07, 3, 0.07, 6, 0.07])

with c1:
    with st.form("instruction_form1", clear_on_submit=False):
        background_prompt = st.text_area(
            "ë°°ê²½ì§€ì‹/ì—­í•  ë¶€ì—¬í•˜ê¸°",
            disabled=False,
            height=250,
            key="background_prompt",
        )

        background_space = st.empty()

        b_0, b_1, b_2 = st.columns([10, 5, 4])

        with b_1:
            background_clear = st.form_submit_button(label="ì´ˆê¸°í™”")
        if background_clear:
            background_space.write(
                "<center> ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ¤– </center>", unsafe_allow_html=Tru
            e)
            if submit_cot and cot_prompt:
                st.session_state["messages"] = [
                    {"role": "system", "content": cot_prompt}
                ]
            else:
                st.session_state["messages"] = []

        with b_2:
            submit_background = st.form_submit_button(label="ì…ë ¥")
        if submit_background:
            background_space.write(
                "<center>ì •ë³´ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ¤–</center>", unsafe_allow_html=Tru
            e)
            st.session_state["messages"].append(
                {"role": "system", "content": background_prompt}
            )

    with st.form("instruction_form2", clear_on_submit=False):
        cot_prompt = st.text_area(
            "ìƒê°ì˜ ì—°ì‡„ ì…ë ¥í•˜ê¸°",
            disabled=False,
            height=200,
            key="cot_prompt",
        )

        cot_space = st.empty()

        cot_0, cot_1, cot_2 = st.columns([10, 5, 4])

        with cot_1:
            cot_clear = st.form_submit_button(label="ì´ˆê¸°í™”")
        if cot_clear:
            cot_space.write("<center> ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ¤– </center>", unsafe_allow_html=True)
            if submit_background and background_prompt:
                mesages = [{"role": "system", "content": background_prompt}]
            else:
                st.session_state["messages"] = []

        with cot_2:
            submit_cot = st.form_submit_button(label="ì…ë ¥")
        if submit_cot:
            cot_space.write("<center>ì •ë³´ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ¤–</center>", unsafe_allow_html=True)
            st.session_state["messages"].append(
                {"role": "system", "content": cot_prompt}
            )


with c2:
    running = False
    with st.form("submit_form", clear_on_submit=True):
        user_input = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", key="input", disabled=running)
        p0, p1 = st.columns([20, 3])
        with p1:
            submit = st.form_submit_button(label="ì…ë ¥")

    loading_text_space = st.empty()

    if submit:
        with loading_text_space:
            st.write("<center>ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...â°</center>", unsafe_allow_html=True)

        for i in range(len(st.session_state["generated"])):
            st.session_state["messages"].append(
                {"role": "user", "content": st.session_state["past"][i]}
            )
            st.session_state["messages"].append(
                {"role": "assistant", "content": st.session_state["generated"][i]}
            )

        st.session_state["messages"].append({"role": "user", "content": user_input})

        running = True

        if st.session_state["api_type"] == "open_ai":
            res = client.chat.completions.create(
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                stream=True,
                messages=st.session_state["messages"],
                model=model_option,
            )

        result = ""

        with st.empty():
            for x in res:
                if "content" in x.choices[0].delta.keys():
                    result += x.choices[0].delta.content

        st.session_state["past"].append(user_input)
        st.session_state["generated"].append(result)

    if st.session_state["generated"]:
        for i in range(len(st.session_state["generated"]) - 1, -1, -1):
            message(
                st.session_state["past"][i],
                avatar_style="fun-emoji",
                is_user=True,
                key=str(i) + "_user",
            )
            message(
                st.session_state["generated"][i],
                avatar_style="thumbs",
                is_user=False,
                key=str(i),
                seed=123,
            )

        loading_text_space.write("")
